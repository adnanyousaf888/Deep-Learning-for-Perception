{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40KWTutBRyAX",
        "outputId": "eb06c45c-41b0-4ed9-d5c2-7f9ea237d136"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import words\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import string\n",
        "import re\n",
        "\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "jRRs6F8kRemY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"race\", \"all\")\n",
        "\n",
        "# Extracting sentences\n",
        "def extract_sentences(data):\n",
        "    sentences = []\n",
        "    for sample in data:\n",
        "        passage = sample['article']\n",
        "        sentences.extend(passage.split('.'))\n",
        "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "train_sentences = extract_sentences(dataset['train'])\n",
        "validation_sentences = extract_sentences(dataset['validation'])\n",
        "sentences = train_sentences\n",
        "#  + validation_sentences"
      ],
      "metadata": {
        "id": "-ROxyvBbRfDz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sentences[:10000]"
      ],
      "metadata": {
        "id": "BWaK2SU0SA7k"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if a word is valid\n",
        "def is_valid_word(word):\n",
        "    english_vocab = set(words.words())\n",
        "    return word.lower() in english_vocab and word.isalpha()"
      ],
      "metadata": {
        "id": "F9zl-2bXRfGO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "#Hugging Face tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "#tokenizer's vocabulary once\n",
        "vocab_set = set(tokenizer.get_vocab().keys())\n",
        "\n",
        "def is_valid_word(word):\n",
        "    return word.isalpha() and word in vocab_set\n",
        "\n",
        "def create_fill_in_blank_data(sentences, batch_size=32):\n",
        "    data = []\n",
        "\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch = sentences[i:i + batch_size]\n",
        "\n",
        "        # Tokenizing all sentences in the batch\n",
        "        tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in batch]\n",
        "\n",
        "        for words_in_sentence in tokenized_sentences:\n",
        "            # Split into halves\n",
        "            mid_index = len(words_in_sentence) // 2\n",
        "            latter_half = words_in_sentence[mid_index:]\n",
        "\n",
        "            # Skip iflatter half is too short\n",
        "            if len(latter_half) < 2:\n",
        "                continue\n",
        "\n",
        "            #finding valid blank words\n",
        "            valid_candidates = [i for i, word in enumerate(latter_half) if is_valid_word(word)]\n",
        "            if not valid_candidates:\n",
        "                continue\n",
        "\n",
        "            #selecting a random valid blank word\n",
        "            blank_index = random.choice(valid_candidates)\n",
        "            blank_word = latter_half[blank_index]\n",
        "\n",
        "            # Replacing blank word with a placeholder\n",
        "            latter_half[blank_index] = \"[BLANK]\"\n",
        "\n",
        "            # Append processed sentence parts and the blank word to the dataset\n",
        "            data.append({\n",
        "                \"part_a\": \" \".join(words_in_sentence[:mid_index]),\n",
        "                \"part_b\": \" \".join(latter_half),\n",
        "                \"part_b_reversed\": \" \".join(reversed(latter_half)),\n",
        "                \"blank_word\": blank_word,\n",
        "            })\n",
        "\n",
        "    return data\n",
        "#calling function\n",
        "preprocessed_data = create_fill_in_blank_data(sentences)\n"
      ],
      "metadata": {
        "id": "_HMkS7o6RfI5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "train_data, val_data = train_test_split(preprocessed_data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "21OmOZSMRfLA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning blank words\n",
        "def clean_blank_word(blank_word):\n",
        "    cleaned_word = blank_word.strip(string.punctuation + \" \")\n",
        "    if not cleaned_word or cleaned_word in string.punctuation or re.fullmatch(r'\\d+', cleaned_word):\n",
        "        return None\n",
        "    if re.fullmatch(r'[\\d-]+', cleaned_word):\n",
        "        return None\n",
        "    return cleaned_word"
      ],
      "metadata": {
        "id": "yjBgXlvxRfM-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "max_len = 50\n",
        "\n",
        "# Dataset class\n",
        "class FillBlankDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        input_a = tokenizer(item['part_a'], padding=\"max_length\", truncation=True, max_length=max_len, return_tensors=\"pt\")['input_ids'].squeeze()\n",
        "        input_b = tokenizer(item['part_b_reversed'], padding=\"max_length\", truncation=True, max_length=max_len, return_tensors=\"pt\")['input_ids'].squeeze()\n",
        "        cleaned_blank_word = clean_blank_word(item['blank_word'])\n",
        "        if cleaned_blank_word is None:\n",
        "            return None\n",
        "        target_tokens = tokenizer(cleaned_blank_word, add_special_tokens=False, return_tensors=\"pt\")['input_ids'].squeeze()\n",
        "        target = target_tokens[0].item() if target_tokens.dim() > 0 else target_tokens.item()\n",
        "        return {\"input_a\": input_a, \"input_b\": input_b, \"target\": target}"
      ],
      "metadata": {
        "id": "V53JvORLRfPB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader\n",
        "train_dataset = FillBlankDataset(train_data)\n",
        "val_dataset = FillBlankDataset(val_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: list(filter(None, x)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=lambda x: list(filter(None, x)))"
      ],
      "metadata": {
        "id": "C4d3Wdzra0jL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Model\n",
        "class LSTMModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pad_idx):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
        "        self.lstm = torch.nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        return self.fc(lstm_out[:, -1, :])"
      ],
      "metadata": {
        "id": "PQ1T9oZUa0lj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models and optimizers\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "embed_size = 128\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "pad_idx = tokenizer.pad_token_id\n",
        "\n",
        "forward_lstm = LSTMModel(vocab_size, embed_size, hidden_size, num_layers, pad_idx).to(\"cuda\")\n",
        "backward_lstm = LSTMModel(vocab_size, embed_size, hidden_size, num_layers, pad_idx).to(\"cuda\")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "forward_optimizer = torch.optim.Adam(forward_lstm.parameters(), lr=0.001)\n",
        "backward_optimizer = torch.optim.Adam(backward_lstm.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "23BQgy96a0n5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training func\n",
        "def train_model(model, optimizer, dataloader, input_key):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        inputs = torch.stack([item[input_key] for item in batch]).to(\"cuda\")\n",
        "        targets = torch.tensor([item[\"target\"] for item in batch]).to(\"cuda\")\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "dxGlzniOa0p9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating func\n",
        "def evaluate_model(model, dataloader, input_key):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = torch.stack([item[input_key] for item in batch]).to(\"cuda\")\n",
        "            targets = torch.tensor([item[\"target\"] for item in batch]).to(\"cuda\")\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "gEdawMXga8MM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9NsZCaNRCMj",
        "outputId": "09c88efa-ee42-4887-fa01-3e10a52d5bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "  Forward LSTM Loss: 7.8727, Accuracy: 0.0201\n",
            "  Backward LSTM Loss: 7.8644, Accuracy: 0.0592\n",
            "Epoch 2:\n",
            "  Forward LSTM Loss: 6.7643, Accuracy: 0.0592\n",
            "  Backward LSTM Loss: 6.7656, Accuracy: 0.0592\n",
            "Epoch 3:\n",
            "  Forward LSTM Loss: 6.6623, Accuracy: 0.0592\n",
            "  Backward LSTM Loss: 6.6654, Accuracy: 0.0592\n",
            "Epoch 4:\n",
            "  Forward LSTM Loss: 6.6349, Accuracy: 0.0592\n",
            "  Backward LSTM Loss: 6.6371, Accuracy: 0.0592\n",
            "Epoch 5:\n",
            "  Forward LSTM Loss: 6.6228, Accuracy: 0.0592\n",
            "  Backward LSTM Loss: 6.6237, Accuracy: 0.0592\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(5):\n",
        "    forward_loss = train_model(forward_lstm, forward_optimizer, train_loader, \"input_a\")\n",
        "    backward_loss = train_model(backward_lstm, backward_optimizer, train_loader, \"input_b\")\n",
        "\n",
        "    forward_acc = evaluate_model(forward_lstm, val_loader, \"input_a\")\n",
        "    backward_acc = evaluate_model(backward_lstm, val_loader, \"input_b\")\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}:\")\n",
        "    print(f\"  Forward LSTM Loss: {forward_loss:.4f}, Accuracy: {forward_acc:.4f}\")\n",
        "    print(f\"  Backward LSTM Loss: {backward_loss:.4f}, Accuracy: {backward_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j8p_xLhyaywx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus**"
      ],
      "metadata": {
        "id": "l5jV43ZgTY0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#Bidirectional LSTM and GRU\n",
        "class BidirectionalLSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pad_idx):\n",
        "        super(BidirectionalLSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        return self.fc(lstm_out[:, -1, :])  # Output for last timestep"
      ],
      "metadata": {
        "id": "Ucs1Cw-FbDvq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pad_idx):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        gru_out, _ = self.gru(embedded)\n",
        "        return self.fc(gru_out[:, -1, :])  # Output for last timestep"
      ],
      "metadata": {
        "id": "JXOIbvL0bDyN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize alternate models\n",
        "bidirectional_lstm = BidirectionalLSTMModel(vocab_size, embed_size, hidden_size, num_layers, pad_idx).to(\"cuda\")\n",
        "gru_model = GRUModel(vocab_size, embed_size, hidden_size, num_layers, pad_idx).to(\"cuda\")\n",
        "\n",
        "# Optimizers\n",
        "bilstm_optimizer = optim.Adam(bidirectional_lstm.parameters(), lr=0.001)\n",
        "gru_optimizer = optim.Adam(gru_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "1E77t5rUbD0w"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble Prediction\n",
        "def ensemble_prediction(forward_output, backward_output):\n",
        "    alpha = 0.5  # Weight for forward model\n",
        "    beta = 0.5   # Weight for backward model\n",
        "\n",
        "    combined_output = alpha * forward_output + beta * backward_output\n",
        "    predictions = torch.argmax(combined_output, dim=1)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "li2u20AqbD2x"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    # Train LSTMs\n",
        "    forward_loss = train_model(forward_lstm, forward_optimizer, train_loader, \"input_a\")\n",
        "    backward_loss = train_model(backward_lstm, backward_optimizer, train_loader, \"input_b\")\n",
        "    bilstm_loss = train_model(bidirectional_lstm, bilstm_optimizer, train_loader, \"input_a\")\n",
        "    gru_loss = train_model(gru_model, gru_optimizer, train_loader, \"input_a\")\n",
        "\n",
        "    # Evaluating LSTMs\n",
        "    forward_acc = evaluate_model(forward_lstm, val_loader, \"input_a\")\n",
        "    backward_acc = evaluate_model(backward_lstm, val_loader, \"input_b\")\n",
        "    bilstm_acc = evaluate_model(bidirectional_lstm, val_loader, \"input_a\")\n",
        "    gru_acc = evaluate_model(gru_model, val_loader, \"input_a\")\n",
        "\n",
        "    # Ensemble Evaluation\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    forward_lstm.eval()\n",
        "    backward_lstm.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_a = torch.stack([item[\"input_a\"] for item in batch]).to(\"cuda\")\n",
        "            input_b = torch.stack([item[\"input_b\"] for item in batch]).to(\"cuda\")\n",
        "            targets = torch.tensor([item[\"target\"] for item in batch]).to(\"cuda\")\n",
        "\n",
        "            forward_outputs = forward_lstm(input_a)\n",
        "            backward_outputs = backward_lstm(input_b)\n",
        "\n",
        "            ensemble_preds = ensemble_prediction(forward_outputs, backward_outputs)\n",
        "            total_correct += (ensemble_preds == targets).sum().item()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "    ensemble_acc = total_correct / total_samples\n",
        "\n",
        "    # Report Metrics\n",
        "    print(f\"Epoch {epoch + 1}:\")\n",
        "    print(f\"  Forward LSTM Loss: {forward_loss:.4f}, Accuracy: {forward_acc:.4f}\")\n",
        "    print(f\"  Backward LSTM Loss: {backward_loss:.4f}, Accuracy: {backward_acc:.4f}\")\n",
        "    print(f\"  Bidirectional LSTM Loss: {bilstm_loss:.4f}, Accuracy: {bilstm_acc:.4f}\")\n",
        "    print(f\"  GRU Loss: {gru_loss:.4f}, Accuracy: {gru_acc:.4f}\")\n",
        "    print(f\"  Ensemble Accuracy: {ensemble_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL10NNQJTJNE",
        "outputId": "3f767974-09b8-4017-ba50-4966bce6039a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "  Forward LSTM Loss: 6.6153, Accuracy: 0.0592\n",
            "  Backward LSTM Loss: 6.6173, Accuracy: 0.0592\n",
            "  Bidirectional LSTM Loss: 7.8534, Accuracy: 0.0592\n",
            "  GRU Loss: 7.9930, Accuracy: 0.0592\n",
            "  Ensemble Accuracy: 0.0592\n",
            "Epoch 2:\n",
            "  Forward LSTM Loss: 6.6086, Accuracy: 0.0592\n",
            "  Backward LSTM Loss: 6.6127, Accuracy: 0.0592\n",
            "  Bidirectional LSTM Loss: 6.7644, Accuracy: 0.0592\n",
            "  GRU Loss: 6.9115, Accuracy: 0.0592\n",
            "  Ensemble Accuracy: 0.0592\n",
            "Epoch 3:\n",
            "  Forward LSTM Loss: 6.6076, Accuracy: 0.0592\n",
            "  Backward LSTM Loss: 6.6084, Accuracy: 0.0592\n",
            "  Bidirectional LSTM Loss: 6.6640, Accuracy: 0.0592\n",
            "  GRU Loss: 6.7108, Accuracy: 0.0520\n",
            "  Ensemble Accuracy: 0.0592\n",
            "Epoch 4:\n",
            "  Forward LSTM Loss: 6.6040, Accuracy: 0.0592\n",
            "  Backward LSTM Loss: 6.6049, Accuracy: 0.0592\n",
            "  Bidirectional LSTM Loss: 6.6385, Accuracy: 0.0592\n",
            "  GRU Loss: 6.5142, Accuracy: 0.0443\n",
            "  Ensemble Accuracy: 0.0592\n",
            "Epoch 5:\n",
            "  Forward LSTM Loss: 6.6012, Accuracy: 0.0592\n",
            "  Backward LSTM Loss: 6.6040, Accuracy: 0.0592\n",
            "  Bidirectional LSTM Loss: 6.6241, Accuracy: 0.0592\n",
            "  GRU Loss: 6.2091, Accuracy: 0.0515\n",
            "  Ensemble Accuracy: 0.0592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hCwOB4yITJpb"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}